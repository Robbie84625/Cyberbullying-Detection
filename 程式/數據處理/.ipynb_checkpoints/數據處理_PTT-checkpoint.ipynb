{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f32ae84c",
   "metadata": {},
   "source": [
    "## 匯入套件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc4b093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "import re\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c4d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhon.hanzi import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import word2vec\n",
    "from pandas import json_normalize\n",
    "from ckiptagger import WS, POS, NER\n",
    "from ckiptagger import construct_dictionary\n",
    "from warnings import simplefilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0719652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a641b6",
   "metadata": {},
   "source": [
    "## 資料夾內容，存成list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baf891c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997003b9",
   "metadata": {},
   "source": [
    "## 讀取所有json檔案，存成csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0fcbd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "##資料夾路徑\n",
    "path='..\\..\\data統整\\PTT\\PTT_JASON_檔案\\標記答案_最終答案'\n",
    "json_data_list=[]\n",
    "\n",
    "#遍歷路徑中資料夾，存成list\n",
    "for file in os.listdir(path):\n",
    "    \n",
    "    #判斷子檔案的後綴字詞是否為.json\n",
    "    if file.endswith('.json'):\n",
    "        #將path與file找到檔案字串進行串接\n",
    "        json_data=os.path.join(path,file)\n",
    "        #讀取json檔\n",
    "        load_json=json.load(codecs.open(json_data, 'r', 'utf-8-sig'))\n",
    "        #萃取key為推文的部分，存成json_tweets\n",
    "        json_tweets= load_json['推文']\n",
    "        context_list=[]\n",
    "        answer_list=[]\n",
    "        new_dict={}\n",
    "        \n",
    "        #迴圈依序挑出字典的value\n",
    "        for i in range(1, len(json_tweets)+1):\n",
    "            \n",
    "            #分別萃取出字典1到最後的value，在挑出下層字典的留言內容的value與答案的value\n",
    "            context_list.append(json_tweets[str(i)]['留言內容'])\n",
    "            answer_list.append(json_tweets[str(i)]['答案'])\n",
    "            #以字典形式儲存\n",
    "            new_dict={'留言內容':context_list,'答案':answer_list}\n",
    "            \n",
    "        \n",
    "        #df為字典構成的dataframe   \n",
    "        df = pd.DataFrame.from_dict(new_dict)\n",
    "        #將dataframe新增到list(all_data_list)\n",
    "        json_data_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647eb1e",
   "metadata": {},
   "source": [
    "## Json檔串接完保留成CSV檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094842fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#將list以concat的方式進行串接\n",
    "all_json_data=pd.concat(json_data_list)\n",
    "#存成csv檔\n",
    "all_json_data.to_csv (r\"..\\..\\data統整\\PTT\\context.csv\", index = False, header=True,encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9baa658",
   "metadata": {},
   "source": [
    "## 刪除URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0470b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RemoveUrl=[]\n",
    "for word,answer in zip(all_json_data['留言內容'],all_json_data['答案']):\n",
    "    if \".html\" not in word:\n",
    "        word=re.sub(r\"http\\S+\",\"\", str(word))\n",
    "        word=re.sub(r\"goo.gl\\S+\",\"\", str(word))\n",
    "        RemoveUrl.append([word,answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe78074a",
   "metadata": {},
   "source": [
    "## 載入停用詞表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab254bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_data =pd.read_csv(r\"..\\..\\字典\\停用詞\\stopword-extended.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e3827",
   "metadata": {},
   "source": [
    "## 清除空格，拆分文本和內容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "506245e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "answer_list=[]\n",
    "for word,answer in RemoveUrl:\n",
    "    word= word.replace(' ', '')\n",
    "    word_list.append(word.lower())\n",
    "    answer_list.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bdb90",
   "metadata": {},
   "source": [
    "## 情感字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7863e65",
   "metadata": {},
   "source": [
    "#### 匯入情感字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caafba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "S_d  = pd.read_csv(r\"..\\..\\字典\\情感字典\\ChineseEmoBank\\CVAW_SD\\CVAW_all_SD.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f94899",
   "metadata": {},
   "source": [
    "#### 新增一個空的CKIP字典，將情感詞導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df081f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_CKIP={}\n",
    "dict_for_CKIP = dict((el,1) for el in S_d['Word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8498843",
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_dict={}\n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "for sentence in RemoveUrl:\n",
    "    #假設表情字串存在表情符號\n",
    "    emoji_temp=emot_obj.emoji(sentence)\n",
    "    if len(emoji_temp['value'])!=0:\n",
    "        for word in emoji_temp['value']:\n",
    "            emot_dict[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cafb7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_CKIP.update(emot_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acb473",
   "metadata": {},
   "source": [
    "## word2Vec霸凌詞 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff78433",
   "metadata": {},
   "source": [
    "#### 載入word2Vec霸凌詞字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631807fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec=pd.read_csv(r\"..\\..\\字典\\霸凌字典\\word2Vec.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74465b70",
   "metadata": {},
   "source": [
    "#### word2Vec霸凌詞字典整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e15aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec_word=[]\n",
    "for word in word2Vec['all_20']:\n",
    "    word2Vec_word.extend(word.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdc62f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec_dict = dict((el,1) for el in word2Vec_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bffa5e8",
   "metadata": {},
   "source": [
    "#### 更新CKIP字典(word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "facc909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_CKIP.update(word2Vec_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd609e7",
   "metadata": {},
   "source": [
    "## 卡方霸凌詞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ce4409",
   "metadata": {},
   "source": [
    "#### 載入卡方霸凌詞字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf0a713",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare=pd.read_csv(r\"..\\..\\字典\\霸凌字典\\chisquare_news100.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c86e5",
   "metadata": {},
   "source": [
    "#### 卡方霸凌詞字典整理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "359a8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare_list=[]\n",
    "for word in chisquare['字']:\n",
    "    chisquare_list.extend(word.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8a43047",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_chisquare = dict((el,1) for el in chisquare_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e38a89",
   "metadata": {},
   "source": [
    "#### 更新要匯入CKIP字典的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c6091c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_CKIP.update(dict_chisquare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3a89fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_CKIP = construct_dictionary(dict_for_CKIP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807d789c",
   "metadata": {},
   "source": [
    "## CKIP斷詞、詞性分析、專有名詞辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "707f5a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\anaconda3\\envs\\python37\\lib\\site-packages\\ckiptagger\\model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "C:\\Users\\Yang\\anaconda3\\envs\\python37\\lib\\site-packages\\ckiptagger\\model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "C:\\Users\\Yang\\anaconda3\\envs\\python37\\lib\\site-packages\\ckiptagger\\model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "ws = WS(r\"..\\data\")\n",
    "pos = POS(\"../data\")\n",
    "ner = NER(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27158894",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_result_temp = ws(word_list,coerce_dictionary = dict_for_CKIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3156d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_results=[]\n",
    "for sentence in  ws_result_temp:\n",
    "    ws_temp=[]\n",
    "    for word in sentence:\n",
    "        word= word.replace(' ', '')\n",
    "        if len(word) !=0:\n",
    "            ws_temp.append(word)\n",
    "    ws_results.append(ws_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178aa4b0",
   "metadata": {},
   "source": [
    "## 詞性與命名實體標註"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daf8d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_results = pos(ws_result_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94feeab0",
   "metadata": {},
   "source": [
    "## 刪除停用詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ecd34839",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=[]\n",
    "for sentence in ws_results:\n",
    "    ws_temp=[]\n",
    "    for word in sentence:\n",
    "        word= word.translate(str.maketrans('', '', punctuation))\n",
    "        word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "        pattern = re.compile(u'[^0-9a-zA-Zａ-ｚＡ-Ｚ\\u4e00-\\u9fa5\\u3100-\\u312F\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF.，,。？\"\"]+')\n",
    "        if len(word)!=0 and word not in stopword_data['停用詞'].tolist():\n",
    "            ws_temp.append(word)\n",
    "    temp.append(ws_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "52c660d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1=[]\n",
    "for sentence in ws_results:\n",
    "    ws_temp1=[]\n",
    "    for word in sentence:\n",
    "        word= word.translate(str.maketrans('', '', punctuation))\n",
    "        word= word.translate(str.maketrans('', '', string.punctuation))\n",
    "        pattern = re.compile(u'[^0-9a-zA-Zａ-ｚＡ-Ｚ\\u4e00-\\u9fa5\\u3100-\\u312F\\u3040-\\u309F\\u30A0-\\u30FF\\u31F0-\\u31FF.，,。？\"\"]+')\n",
    "        word=re.sub(pattern,\"\", str(word))\n",
    "        if len(word)!=0 and word not in stopword_data['停用詞'].tolist():\n",
    "            ws_temp1.append(word)\n",
    "    temp1.append(ws_temp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351ce11d",
   "metadata": {},
   "source": [
    "#### 清除刪完停用詞後的空的評論\n",
    "remove_stw_ws：刪完停用詞文本，remove_stw_ans：文本標註，lenContent：評論長度，ws_data：不刪停用詞文本，pos_data：不刪停用詞詞性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae6ff761",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stw_ws=[]\n",
    "ws_data=[]\n",
    "pos_data=[]\n",
    "remove_stw_ans=[]\n",
    "remove_stw_count=[]\n",
    "\n",
    "for rsw_word,word,pos,answer in zip(temp,ws_results,pos_results,answer_list):\n",
    "    if len(rsw_word) !=0:\n",
    "        remove_stw_ws.append(rsw_word)\n",
    "        ws_data.append(word)\n",
    "        pos_data.append(pos)\n",
    "        remove_stw_ans.append(answer)\n",
    "        remove_stw_count.append(len(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41631cc2",
   "metadata": {},
   "source": [
    "## 詞袋模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446c756",
   "metadata": {},
   "source": [
    "#### 建立字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fdcb36c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(remove_stw_ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19872a28",
   "metadata": {},
   "source": [
    "以迴圈遍歷斷詞後的list，取出所有分詞  \n",
    "將所有分詞結果放入 dictionary.doc2bow  \n",
    "dictionary.doc2bow(word) 裡的 word 是每一句評論的斷詞結果  \n",
    "dictionary.doc2bow(word)方法將斷詞結果以詞袋模型展現  \n",
    "將結果放入list當中  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d492b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_bag = [dictionary.doc2bow(word) for word in remove_stw_ws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5b634820",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_words = [[(dictionary[id], count) for id, count in line] for line in words_bag]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c391428",
   "metadata": {},
   "source": [
    "#### 取出所有詞彙整成list，並刪除重複值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b0b077e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_list = list(set(np.concatenate(remove_stw_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cccbf",
   "metadata": {},
   "source": [
    "## 建立空的Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "587e275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_DF = np.zeros((len(id_words),len(ws_list)))\n",
    "BOW_DF = pd.DataFrame(data = zero_DF, columns=ws_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00bed6",
   "metadata": {},
   "source": [
    "## 將單詞次數寫入Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb403c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,id_word in enumerate(id_words):\n",
    "    for word,count in id_word:\n",
    "        BOW_DF.loc[index, word] =count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87839cbb",
   "metadata": {},
   "source": [
    "#### 將答案的list插入至Bag of words向量，存成csv檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edf6ebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_DF=BOW_DF.assign(answer=remove_stw_ans)\n",
    "BOW_DF.to_csv (r\"..\\..\\Feature_Array\\PTT\\Main_Features\\Bag_of_Word.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab0f14",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6a410",
   "metadata": {},
   "source": [
    "#### 做出一個以空格斷詞的 list 方便跑TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3e40a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_ws=[]\n",
    "for word in remove_stw_ws:\n",
    "    sentence=' '.join(word)\n",
    "    join_ws.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc8050c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(max_feature):\n",
    "    vectoriser = TfidfVectorizer(norm = None,token_pattern=r\"(?u)\\b\\w+\\b\",max_features=max_feature)\n",
    "    tf_idf_scores = vectoriser.fit_transform(join_ws)\n",
    "    feature_names = vectoriser.get_feature_names()\n",
    "    df_tf_idf = pd.DataFrame(tf_idf_scores.todense(), columns =  feature_names)\n",
    "    return df_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aff65da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf_1000=TF_IDF(1000)\n",
    "df_tf_idf_500=TF_IDF(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3549fd",
   "metadata": {},
   "source": [
    "#### 將答案的list插入至TF-IDF向量，存成csv檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f5eb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf_1000=df_tf_idf_1000.assign(answer=remove_stw_ans)\n",
    "df_tf_idf_1000.to_csv (r\"..\\..\\Feature_Array\\PTT\\Main_Features\\TF-IDF_1000字.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "06aa714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tf_idf_500=df_tf_idf_500.assign(answer=remove_stw_ans)\n",
    "df_tf_idf_500.to_csv (r\"..\\..\\Feature_Array\\PTT\\Main_Features\\TF-IDF_500字.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf408a5",
   "metadata": {},
   "source": [
    "## 計算情感分數"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd5565",
   "metadata": {},
   "source": [
    "#### 製作情感字典(key=詞:value=情感分數)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "633411f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_dict =  dict((word,value) for word,value in zip(S_d ['Word'], S_d ['Valence_Mean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ae4996",
   "metadata": {},
   "source": [
    "#### 包含 文本、正向情感字數、負向情感字數、情感分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d601be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_list=[]\n",
    "for sentence,count in zip(ws_data,remove_stw_count):\n",
    "    p=0\n",
    "    n=0\n",
    "    sentiment_score=0\n",
    "    for word in sentence:\n",
    "        s=' '.join(sentence) \n",
    "        #在情感字典裡且情感分數>5為正向情感字\n",
    "        if word in Sentiment_dict.keys() and Sentiment_dict[word]>5:\n",
    "            p=(p+1)\n",
    "         #在情感字典裡且情感分數<5為負向情感字\n",
    "        if word in Sentiment_dict.keys() and Sentiment_dict[word]<5:\n",
    "            n=(n+1)\n",
    "        if word in Sentiment_dict.keys():\n",
    "            sentiment_score=sentiment_score+(((Sentiment_dict[word]-5)/4)+1)\n",
    "    #計算正向情感字比率 正向字字數/總字數        \n",
    "    p=p/count\n",
    "    #計算負向情感字比率 負向字字數/總字數        \n",
    "    n=n/count\n",
    "    \n",
    "    Sentiment={'文本':s,'正向情感比率':p,'負向情感比率':n,'情感分數':sentiment_score}\n",
    "    df=pd.DataFrame([Sentiment])\n",
    "    Sentiment_list.append(df)\n",
    "Sentiment_data=pd.concat(Sentiment_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58c71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment_data.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Sentiment_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc0854",
   "metadata": {},
   "source": [
    "## word2Vec霸凌詞分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f873aae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bully_list=[]\n",
    "for sentence,count in zip(ws_data,remove_stw_count):\n",
    "    bully_count=0\n",
    "    for word in sentence:\n",
    "        s=' '.join(sentence) \n",
    "        #假如霸凌詞有在list裡面\n",
    "        if str(word) in word2Vec_word:\n",
    "            bully_count+=1\n",
    "    bully_count=bully_count/count\n",
    "    bully_dict={'文本':s,'霸凌詞比率':bully_count}\n",
    "    df=pd.DataFrame([bully_dict])\n",
    "    bully_list.append(df)\n",
    "    \n",
    "bully_data=pd.concat(bully_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a219ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "bully_data.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Word2Vec_Bully_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cdb82",
   "metadata": {},
   "source": [
    "## 卡方霸凌詞分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62ced57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare_list=[]\n",
    "for sentence,count in zip(ws_data,remove_stw_count):\n",
    "    bully_count=0\n",
    "    for word in sentence:\n",
    "        s=' '.join(sentence) \n",
    "        #假如霸凌詞有在list裡面\n",
    "        if str(word) in chisquare['字'].tolist():\n",
    "            bully_count+=1\n",
    "    bully_count=bully_count/count\n",
    "    bully_dict={'文本':s,'霸凌詞比率':bully_count}\n",
    "    df=pd.DataFrame([bully_dict])\n",
    "    chisquare_list.append(df)\n",
    "\n",
    "chisquare_data=pd.concat(chisquare_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "423bb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare_data.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Chisquare_Bully_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d41ae",
   "metadata": {},
   "source": [
    "## 代詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0225cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun=pd.read_csv(r\"..\\..\\字典\\詞性字典\\人稱代詞.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e8dffe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_list=[]\n",
    "for sentence,count in zip(ws_data,remove_stw_count):\n",
    "    first=0\n",
    "    second=0\n",
    "    third=0\n",
    "    for word in sentence:\n",
    "        s=' '.join(sentence)\n",
    "        if word in pronoun['第一人稱'].tolist():\n",
    "            first+=1\n",
    "        if word in pronoun['第二人稱'].tolist():\n",
    "            second+=1\n",
    "        if word in pronoun['第三人稱'].tolist():\n",
    "            third+=1\n",
    "\n",
    "    first=first/count     \n",
    "    second=second/count\n",
    "    third=third/count\n",
    "\n",
    "    \n",
    "    pronoun_dict={'文本':s,'第一人稱比率':first,'第二人稱比率':second,'第三人稱比率':third}\n",
    "    df=pd.DataFrame([pronoun_dict])\n",
    "    pronoun_list.append(df)\n",
    "pronoun_data=pd.concat(pronoun_list)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54e89266",
   "metadata": {},
   "outputs": [],
   "source": [
    "pronoun_data.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Pronoun_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dafe14d",
   "metadata": {},
   "source": [
    "### 統計特徵(留言長度+人稱代詞)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7064833e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistical_Features=pronoun_data.assign(留言長度= remove_stw_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9dc3b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistical_Features.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Statistical_Features.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e27666b",
   "metadata": {},
   "source": [
    "## 詞性統計(48類)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a787253",
   "metadata": {},
   "source": [
    "#### 創建一個{key=所有詞性:value=0}的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a3f588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list= list(set(np.concatenate(pos_data)))\n",
    "pos_list.sort()\n",
    "\n",
    "pos_dict={}\n",
    "\n",
    "for i in pos_list:\n",
    "    pos_dict[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6471e166",
   "metadata": {},
   "source": [
    "#### 創建一個空的詞性DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa85018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_zero_df = np.zeros((len(pos_data),len(pos_list)))\n",
    "pos_df = pd.DataFrame(data = pos_zero_df, columns=pos_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82c02f",
   "metadata": {},
   "source": [
    "#### 遍歷文本，將統計數值填入DataFrame當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "410afbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,list_pos in  enumerate(pos_data):\n",
    "    for pos in list_pos:\n",
    "        if pos in pos_dict.keys():\n",
    "            pos_dict[pos]+=1\n",
    "            pos_df.loc[index, pos]=pos_dict[pos]/len(list_pos)\n",
    "    for i in pos_dict:\n",
    "        pos_dict[i]=0            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b5fc26",
   "metadata": {},
   "source": [
    "#### 存成csv檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3654bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\POS_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af8e08",
   "metadata": {},
   "source": [
    "## 詞性統計(11類)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a5333",
   "metadata": {},
   "source": [
    "#### 匯入分類的詞性表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b595e1b7",
   "metadata": {},
   "source": [
    "#### 創建一個{key=分類後詞性項目:value=0}的字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41c52e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_table=pd.read_csv(r\"..\\..\\字典\\詞性字典\\詞性表.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "410d3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_pos_dict={}\n",
    "\n",
    "for i in pos_table.columns:\n",
    "    simple_pos_dict[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa969bc5",
   "metadata": {},
   "source": [
    "#### 創建一個空的10類詞性DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8d37f33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_simple_data = np.zeros((len(pos_data),len(simple_pos_dict)))\n",
    "pos_simple_df = pd.DataFrame(data = pos_simple_data, columns=simple_pos_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4b08e",
   "metadata": {},
   "source": [
    "#### 遍歷文本，將統計數值填入DataFrame當中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f90038f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,pos_sentence in enumerate(pos_data):\n",
    "    for pos in pos_sentence:\n",
    "        if pos in pos_table['A'].tolist():\n",
    "            simple_pos_dict['A']+=1\n",
    "            pos_simple_df.loc[index, 'A'] =simple_pos_dict['A']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['C'].tolist():\n",
    "            simple_pos_dict['C']+=1\n",
    "            pos_simple_df.loc[index, 'C'] =simple_pos_dict['C']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['D'].tolist():\n",
    "            simple_pos_dict['D']+=1\n",
    "            pos_simple_df.loc[index, 'D'] =simple_pos_dict['D']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['FW'].tolist():\n",
    "            simple_pos_dict['FW']+=1\n",
    "            pos_simple_df.loc[index, 'FW'] =simple_pos_dict['FW']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['I'].tolist():\n",
    "            simple_pos_dict['I']+=1\n",
    "            pos_simple_df.loc[index, 'I'] =simple_pos_dict['I']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['N'].tolist():\n",
    "            simple_pos_dict['N']+=1\n",
    "            pos_simple_df.loc[index, 'N'] =simple_pos_dict['N']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['P'].tolist():\n",
    "            simple_pos_dict['P']+=1\n",
    "            pos_simple_df.loc[index, 'P'] =simple_pos_dict['P']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['SHI'].tolist():\n",
    "            simple_pos_dict['SHI']+=1\n",
    "            pos_simple_df.loc[index, 'SHI'] =simple_pos_dict['SHI']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['T'].tolist():\n",
    "            simple_pos_dict['T']+=1\n",
    "            pos_simple_df.loc[index, 'T'] =simple_pos_dict['T']/len(pos_sentence)\n",
    "            \n",
    "        if pos in pos_table['V'].tolist():\n",
    "            simple_pos_dict['V']+=1\n",
    "            pos_simple_df.loc[index, 'V'] =simple_pos_dict['V']/len(pos_sentence)\n",
    "        \n",
    "        if pos in pos_table['Punctuation'].tolist():\n",
    "            simple_pos_dict['Punctuation']+=1\n",
    "            pos_simple_df.loc[index, 'Punctuation'] =simple_pos_dict['Punctuation']/len(pos_sentence)\n",
    "        \n",
    "    for i in simple_pos_dict.keys():\n",
    "        simple_pos_dict[i]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fd5e51",
   "metadata": {},
   "source": [
    "#### 存成csv檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64b1e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_simple_df.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Simple_POS_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adac1c9",
   "metadata": {},
   "source": [
    "## 表情符號"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59ae81",
   "metadata": {},
   "source": [
    "### 方法一、表情符號情感字典"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff1ce69",
   "metadata": {},
   "source": [
    "#### 導入情感字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5332bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict_data=pd.read_csv(r\"..\\..\\字典\\表情符號字典\\ijstable.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bf164",
   "metadata": {},
   "source": [
    "#### 文本清理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3bd6ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict_data=emoji_dict_data.dropna(subset=['Char'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907f654",
   "metadata": {},
   "source": [
    "#### 建立表情符號字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2faa3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary={}\n",
    "\n",
    "for word,score in zip(emoji_dict_data['Char'],emoji_dict_data['Sentiment score']):\n",
    "    emoji_dictionary[word]=float(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b3a63",
   "metadata": {},
   "source": [
    "#### 計算評論表情符號比率&情感分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fc236a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emot_obj = emot.core.emot() \n",
    "emoji_list=[]\n",
    "emot_list=[]\n",
    "\n",
    "for sentence,count in zip(ws_data,remove_stw_count):\n",
    "    \n",
    "    emoji_count=0\n",
    "    emoji_setiment_value=0.0\n",
    "    \n",
    "    emoji_temp=emot_obj.emoji(sentence)\n",
    "    emot_list.append(emoji_temp)\n",
    "    \n",
    "    if len(emoji_temp['value'])!=0:\n",
    "        for word in emoji_temp['value']:\n",
    "            if  word in emoji_dictionary.keys():\n",
    "                emoji_count+=1\n",
    "                emoji_setiment_value+=emoji_dictionary[word]+1\n",
    "                \n",
    "    emoji_count=emoji_count/count\n",
    "    \n",
    "    emoji_temp_dict={'表情符號比率':emoji_count,'表情符號情感分數':emoji_setiment_value}\n",
    "    df=pd.DataFrame([emoji_temp_dict])\n",
    "    emoji_list.append(df)\n",
    "emoji_data=pd.concat(emoji_list)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8c930ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_data.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Dict_Emoji_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a621f3",
   "metadata": {},
   "source": [
    "#### 情感特徵與表情符號合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60cfcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Sentiment=pd.concat([Sentiment_data.iloc[:, :],emoji_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dae844d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Sentiment.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\All_Sentiment.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c54a1bf",
   "metadata": {},
   "source": [
    "### 方法2、Sentiwordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2acb5f",
   "metadata": {},
   "source": [
    "#### 建立表情符號字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a2cd5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dict={}\n",
    "emot_obj = emot.core.emot() \n",
    "\n",
    "for word in emot_list:\n",
    "    #假設表情字串存在表情符號\n",
    "    if len(word['value'])!=0:\n",
    "        #將其value 和 mean 導入自訂的字典當中，key:value，value：mean\n",
    "        for item_word,item_mean in zip(word['value'],word['mean']):\n",
    "            emoji_dict[item_word]=item_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6048a",
   "metadata": {},
   "source": [
    "#### 以正則表示式清除字典value的冒號"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f0203b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_emoji_word=[]\n",
    "for word in emoji_dict.values():\n",
    "    word=re.sub(r\":\",\"\", str(word))\n",
    "    list_emoji_word.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d8dba",
   "metadata": {},
   "source": [
    "#### 清除字典value中字串中所連接的_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "13398d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_emoji=[]\n",
    "for word in list_emoji_word:\n",
    "    if '_' in word:\n",
    "        w = word.split(\"_\")\n",
    "        list_emoji.append(w)\n",
    "    else:\n",
    "        list_emoji.append([word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271df914",
   "metadata": {},
   "source": [
    "#### 將字詞作詞形還原與關聯詞，計算情感分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dd220f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_score = {}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for i,sentence in enumerate(list_emoji):\n",
    "    word_sentiment = 0.0\n",
    "    for word in sentence:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        if not lemma:\n",
    "            continue\n",
    "        \n",
    "        synsets = wn.synsets(lemma)\n",
    "        if not synsets:\n",
    "            continue\n",
    "        #字典字有限，要找和字典中的字最相關\n",
    "        synset = synsets[0]\n",
    "        swn_synset = swn.senti_synset(synset.name())\n",
    "        word_sentiment += (swn_synset.pos_score() - swn_synset.neg_score())+1\n",
    "\n",
    "    #以i取list_emoji_word的第i個數\n",
    "    #key:emoji的描述，value：情感分數\n",
    "    word_to_score[list_emoji_word[i]] = word_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fa959",
   "metadata": {},
   "source": [
    "#### 建立表情符號字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5fec9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_score_dict={}\n",
    "for key,value in emoji_dict.items():\n",
    "    #把emoji_score_dict的value當word_to_score的key去抓分數\n",
    "    emoji_score_dict[key] = word_to_score[value[1:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f57210",
   "metadata": {},
   "source": [
    "#### 計算評論表情符號比率&情感分數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6e51d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list_2=[]\n",
    "\n",
    "for sentence,count in zip(emot_list,remove_stw_count):\n",
    "    \n",
    "    emoji_count=0\n",
    "    emoji_setiment_value=0.0\n",
    "    \n",
    "    #抓取文本中表情符號\n",
    "    #value：字串中所有表情符號，location：表情符號在字串中的位置 ，mean：表情符號含意，flag：指定字串中是否存在表情符號\n",
    "\n",
    "    #假設表情字串存在表情符號\n",
    "    if len(sentence['value'])!=0:\n",
    "        for word in sentence['value']:\n",
    "            emoji_count+=1\n",
    "            if  word in emoji_score_dict.keys():\n",
    "                emoji_setiment_value+=emoji_score_dict[word]\n",
    "                \n",
    "    emoji_count=emoji_count/count\n",
    "    \n",
    "    emoji_temp_dict={'表情符號比率':emoji_count,'表情符號情感分數':emoji_setiment_value}\n",
    "    df=pd.DataFrame([emoji_temp_dict])\n",
    "    emoji_list_2.append(df)\n",
    "emoji_data_2=pd.concat(emoji_list_2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d6de802",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_data_2.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\Sentiwordnet_Emoji_data.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb77f2e",
   "metadata": {},
   "source": [
    "#### 情感特徵與表情符號合併"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a9f758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Sentiment_2=pd.concat([Sentiment_data.iloc[:, :],emoji_data_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c75d95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Sentiment_2.to_csv (r\"..\\..\\Feature_Array\\PTT\\Extra_Features\\All_Sentiment_Sentiwordnet.csv\", header=True,encoding='utf_8_sig',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
